{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pandas e numpy para trabalhar com Dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importar pyarrow para ler arquivo parquet\n",
    "import pyarrow as pa\n",
    "import  pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho em que se encontra o dataset em csv com a ordem das questoes da prova\n",
    "dfItens = pd.read_csv(\"../../Microdados_enem_2019/DADOS/ITENS_PROVA_2019.csv\", sep=';',encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o dicionário modelo contendo os dataframes vazios para cada area\n",
    "dictSubjects = {\"CN\":[], \"MT\":[], \"CH\":[], \"LC\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o dicionario que será preenchido com a ordem das questoes para cada area\n",
    "dictGabOrder = dictSubjects.copy()\n",
    "\n",
    "# Executar as instruções abaixo para cada uma das areas\n",
    "for subject in dictGabOrder.keys():\n",
    "\n",
    "    # Iniciar o dataframe vazio auxiliar que será populado com colunas contendo \n",
    "    # a ordem das questoes \n",
    "    dfGabOrder = pd.DataFrame()\n",
    "    \n",
    "    # Filtrar o dataset das questoes para a area e \n",
    "    # obter o codigo da prova Azul  \n",
    "    dfSubject = dfItens[dfItens[\"SG_AREA\"]==subject]\n",
    "    blueCode = dfSubject[dfSubject[\"TX_COR\"]==\"Azul\"][\"CO_PROVA\"].unique()[0]\n",
    "\n",
    "    # Executar as instruções abaixo caso a area seja 'LC'\n",
    "    if subject == \"LC\":\n",
    "\n",
    "        # Obter os codigos das questoes para a prova azul da area \n",
    "        # dividir em duas colunas: codigos considerando 'TP_LINGUA' = 0  e  'TP_LINGUA' = 1\n",
    "        dfGabOrder[\"CO_ITEM_0\"] = dfSubject[(dfSubject[\"CO_PROVA\"]==blueCode) & (dfSubject[\"TP_LINGUA\"]!=1)][[\"CO_ITEM\"]].reset_index(drop=True)\n",
    "        dfGabOrder[\"CO_ITEM_1\"] = dfSubject[(dfSubject[\"CO_PROVA\"]==blueCode) & (dfSubject[\"TP_LINGUA\"]!=0)][[\"CO_ITEM\"]].reset_index(drop=True)\n",
    "\n",
    "        # Obter os valores unicos para a combinação 'CO_PROVA' e 'TP_LINGUA'\n",
    "        dfUniqueProvaLingua = dfSubject[[\"CO_PROVA\", \"TP_LINGUA\"]].drop_duplicates().dropna()\n",
    "\n",
    "        # Executar as instrucoes para cada combinacao unica\n",
    "        for code, lingua in dfUniqueProvaLingua.itertuples(index=False):\n",
    "            \n",
    "            # Converter o codigo da lingua 'TP_LINGUA' em int\n",
    "            lingua = int(lingua)\n",
    "            \n",
    "            # Filtrar o dataset da area pelo codigo da prova e lingua + demais questoes\n",
    "            # para obter os codigos das questoes e suas posicoes para aquela prova\n",
    "            dfSubjectFiltered = dfSubject[(dfSubject[\"CO_PROVA\"]==code) & ((dfSubject[\"TP_LINGUA\"]==lingua) | (dfSubject[\"TP_LINGUA\"].isna()))][[\"CO_ITEM\", \"CO_POSICAO\"]]\n",
    "\n",
    "            # Converter as posiçoes para o intervalo 0 a 44 e\n",
    "            # renomear as colunas para que seja identificada o cidogo da lingua\n",
    "            dfSubjectFiltered[\"CO_POSICAO\"] = dfSubjectFiltered[\"CO_POSICAO\"] - np.min(dfSubjectFiltered[\"CO_POSICAO\"])\n",
    "            dfSubjectFiltered.rename(columns= {\"CO_ITEM\":f'CO_ITEM_{lingua}'}, inplace=True) \n",
    "            \n",
    "            # Fazer um merge do dataset fitlrado com o dataset acumulado, \n",
    "            # utilizando como chave o codigo da questao\n",
    "            dfGabOrder = dfGabOrder.merge(dfSubjectFiltered, on=f'CO_ITEM_{lingua}', how=\"left\", suffixes=(None, f'_{code}_{lingua}'))\n",
    "\n",
    "        # Renomear a primeira coluna para adicionar o codigo da prova e da lingua            \n",
    "        dfGabOrder.rename(columns= {\"CO_POSICAO\":f'CO_POSICAO_{blueCode}_{0}'}, inplace=True)\n",
    "\n",
    "        # Salvar o Dataframe no dicionario \n",
    "        dictGabOrder[subject] = dfGabOrder\n",
    "        \n",
    "\n",
    "     # Executar as instruções abaixo caso a area não seja 'LC'\n",
    "    else:\n",
    "        # Obter os codigos das questoes para a prova azul da area \n",
    "        dfGabOrder[\"CO_ITEM\"] = dfSubject[dfSubject[\"CO_PROVA\"]==blueCode][[\"CO_ITEM\"]]\n",
    "\n",
    "        # Executar as instrucoes para cada coodigo de prova\n",
    "        for code in dfSubject['CO_PROVA'].unique():\n",
    "\n",
    "            # Filtrar o dataset da area pelo codigo da prova, para obter\n",
    "            # os codigos das questoes e suas posicoes para aquela prova\n",
    "            dfSubjectFiltered = dfSubject[dfSubject[\"CO_PROVA\"]==code][[\"CO_ITEM\", \"CO_POSICAO\"]]\n",
    "\n",
    "            # Converter as posiçoes para o intervalo 0 a 44\n",
    "            dfSubjectFiltered[\"CO_POSICAO\"] = dfSubjectFiltered[\"CO_POSICAO\"] - np.min(dfSubjectFiltered[\"CO_POSICAO\"])\n",
    "            \n",
    "            # Fazer um merge do dataset fitlrado com o dataset acumulado, \n",
    "            # utilizando como chave o codigo da questao\n",
    "            dfGabOrder = dfGabOrder.merge(dfSubjectFiltered, on=\"CO_ITEM\", how=\"left\", suffixes=(None, f'_{code}'))\n",
    "        \n",
    "         \n",
    "        # Renomear a primeira coluna para adicionar o codigo da prova e da lingua            \n",
    "        dfGabOrder.rename(columns= {\"CO_POSICAO\":f'CO_POSICAO_{blueCode}'}, inplace=True) \n",
    "\n",
    "        # Salvar o Dataframe no dicionario \n",
    "        dictGabOrder[subject] = dfGabOrder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler os arquivo dos datasets contendo as respostas dos candidatos por area\n",
    "dictResp = dictSubjects.copy() \n",
    "\n",
    "for subject in dictResp.keys():\n",
    "    dictResp[subject] = pd.read_parquet(f'../../Datasets/MICRODADOS_ENEM_2019_SAOPAULO_BySubject/{subject}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencher a primeira posicao da coluna 'CO_POSICAO_522', pois existe um erro no dataset original \n",
    "# que nao permitiu a correta identificacao da posicao da questao nessa prova\n",
    "dictGabOrder[\"MT\"].loc[0,\"CO_POSICAO_522\"] = 15\n",
    "\n",
    "# Converter o tipo da coluna para 'int'\n",
    "dictGabOrder[\"MT\"][\"CO_POSICAO_522\"] = dictGabOrder[\"MT\"][\"CO_POSICAO_522\"].astype('int32')\n",
    "\n",
    "# Remover as linhas dos candidatos que fizeram a prova 519 da area CN\n",
    "dictResp['CN'] = dictResp['CN'][dictResp['CN']['CO_PROVA_CN'] != 519].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o dicionario que será preenchido com os arrays contendo a ordem das \n",
    "# questoes para cada linha dos datasets das respostas dos candidatos\n",
    "dictOrderArrays = dictSubjects.copy()\n",
    "\n",
    "# Executar as instruções abaixo para cada uma das areas\n",
    "for subject in dictOrderArrays.keys():\n",
    "    \n",
    "    # Iniciar o array vazio \n",
    "    arrPositions = []\n",
    "\n",
    "    # Iniciar o array com o nome das colunas que serão iteradas \n",
    "    arrColumnsToIterate = ['NU_INSCRICAO', f'CO_PROVA_{subject}']\n",
    "   \n",
    "    # Adicionar a coluna com o codigo da lingua caso a area seja 'LC'\n",
    "    if subject == 'LC':\n",
    "        arrColumnsToIterate.append('TP_LINGUA')\n",
    " \n",
    "    # Executar as instrucoes abaixo para cada linha do dataset de respostas\n",
    "    for index, row in dictResp[subject][arrColumnsToIterate].iterrows():\n",
    "\n",
    "        # Obter o codigo da prova da linha atual\n",
    "        codigo = int(row[f'CO_PROVA_{subject}'])\n",
    "\n",
    "        # Definir o nome da coluna com as posicoes para a prova\n",
    "        co_posicao = f'CO_POSICAO_{codigo}_{int(row[\"TP_LINGUA\"])}' if subject == 'LC' else f'CO_POSICAO_{codigo}'\n",
    "\n",
    "        # Converter a coluna com as posicoes da prova em um array\n",
    "        # e adicionar os demais 'index' que não serão sorteados\n",
    "        arrPos = np.array(dictGabOrder[subject][co_posicao]) + 1 \n",
    "        arrPos = np.insert(arrPos, 0 , 0)\n",
    "        arrPos = np.insert(arrPos, 46 , 46)\n",
    "        arrPos = np.insert(arrPos, 47 , 47)\n",
    "        arrPos = np.insert(arrPos, 48 , 48) if subject == 'LC' else arrPos\n",
    "\n",
    "        # Adicionar o array da linha ao consolidado contendo as demais linhas\n",
    "        arrPositions.append(arrPos)\n",
    "\n",
    "    # Salvar o array consolidado no dicionario\n",
    "    dictOrderArrays[subject] = arrPositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o dicionario que será preenchido com as linhas organizadas do\n",
    "# dataset de respostas\n",
    "dictRespOrdered = dictSubjects.copy()\n",
    "\n",
    "# Executar as instruções abaixo para cada uma das areas\n",
    "for subject in dictRespOrdered.keys():\n",
    "    \n",
    "    # Obter o nome das colunas do dataset de respostas\n",
    "    columns = dictResp[subject].columns\n",
    "\n",
    "    # Criar um dataframe para cada linha do dataframe de respostas e concatenar\n",
    "    # em um dataframe unico no final \n",
    "    dfSubject = pd.concat([ pd.DataFrame(  [row[dictOrderArrays[subject][index]].values], columns=columns )  for index, row in dictResp[subject].iterrows()], ignore_index=True)\n",
    "    \n",
    "    # Salvar o dataframe no dicionario\n",
    "    dictRespOrdered[subject] = dfSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os datasets com as linhas organizadas divididos por area\n",
    "for subject, dfSubject in dictRespOrdered.items():\n",
    "    \n",
    "    table = pa.Table.from_pandas(dfSubject.reset_index(drop=True))\n",
    "\n",
    "    # Definir onde será salvo o novo dataset filtrado por área\n",
    "    path = f'../../Datasets/MICRODADOS_ENEM_2019_SAOPAULO_BySubjectOrdered/{subject}.parquet'\n",
    "\n",
    "    # Gerar um parquet da tabela\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=path,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e774f25fef358570f8855c1820fdc89100b1a0bc15aad31a4e29065a6a0197c3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
